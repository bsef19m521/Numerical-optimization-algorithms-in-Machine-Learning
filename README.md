# Numerical Optimization Algorithms in Machine Learning
This repository contains implementations of various numerical optimization algorithms commonly used in machine learning.

# Optimization Algorithms
The following optimization algorithms have been implemented in this repository:

- Gradient Descent
- Stochastic Gradient Descent
- Mini-batch Gradient Descent
- Adam (Adaptive Moment Estimation)
- Adagrad (Adaptive Gradient Algorithm)
- Adadelta (Adaptive Delta)
- RMSprop (Root Mean Square Propagation)
- Newton's Method


# Getting Started
To use these optimization algorithms, simply download the implementation of the algorithm you want to use and import it into your code. Each implementation comes with a brief explanation of the algorithm and how to use it.

# Contributions
If you have any suggestions or improvements for these implementations, feel free to create a pull request or issue in this repository.

# License
This repository is licensed under the MIT License. See the [LICENSE]() file for more information.
